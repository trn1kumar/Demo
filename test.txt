package com.example;

import com.google.cloud.pubsub.v1.Subscriber;
import com.google.pubsub.v1.*;
import com.google.api.core.ApiService.Listener;
import com.google.api.core.ApiService.State;
import com.google.api.core.ApiService;
import com.google.api.gax.rpc.ApiException;
import com.google.pubsub.v1.PubsubMessage;
import com.google.cloud.pubsub.v1.AckReplyConsumer;

import org.apache.spark.sql.*;

import java.util.*;
import java.util.concurrent.*;

public class PubSubSparkIcebergApp {

    private static final int BATCH_SIZE = 500;
    private static final String ICEBERG_TABLE = "my_catalog.db.my_table";

    private final List<String> buffer = Collections.synchronizedList(new ArrayList<>());

    private final SparkSession spark = SparkSession.builder()
            .appName("PubSubSparkIcebergApp")
            .master("local[*]")
            .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.my_catalog.type", "hive")
            .config("spark.sql.catalog.my_catalog.uri", "thrift://hive-metastore:9083")
            .config("spark.sql.catalog.my_catalog.warehouse", "gs://your-bucket/iceberg-warehouse/")
            .config("spark.sql.catalog.my_catalog.io-impl", "org.apache.iceberg.gcp.gcs.GCSFileIO")
            .getOrCreate();

    public static void main(String[] args) {
        PubSubSparkIcebergApp app = new PubSubSparkIcebergApp();
        app.start("your-gcp-project", "your-subscription-id");
    }

    public void start(String projectId, String subscriptionId) {
        ProjectSubscriptionName subscriptionName = ProjectSubscriptionName.of(projectId, subscriptionId);

        Subscriber subscriber = Subscriber.newBuilder(subscriptionName, this::receiveMessage).build();

        subscriber.addListener(new Listener() {
            @Override
            public void failed(State from, Throwable failure) {
                System.err.println("Subscriber error: " + failure.getMessage());
            }
        }, Executors.newSingleThreadExecutor());

        subscriber.startAsync().awaitRunning();
        System.out.println("Subscriber started...");
    }

    private void receiveMessage(PubsubMessage message, AckReplyConsumer consumer) {
        String msg = message.getData().toStringUtf8();
        buffer.add(msg);
        consumer.ack();

        if (buffer.size() >= BATCH_SIZE) {
            List<String> batch;
            synchronized (buffer) {
                batch = new ArrayList<>(buffer);
                buffer.clear();
            }
            appendToIceberg(batch);
        }
    }

    private void appendToIceberg(List<String> messages) {
        List<Row> rows = new ArrayList<>();
        for (String msg : messages) {
            rows.add(RowFactory.create(msg));
        }

        StructType schema = new StructType().add("message", "string");
        Dataset<Row> df = spark.createDataFrame(rows, schema);

        df.writeTo(ICEBERG_TABLE).append();

        System.out.println("Appended " + messages.size() + " messages to Iceberg");
    }
}
