// Java 17 // Apache Beam + GCS + Spark + Iceberg // Goal: High-speed ingestion from Pub/Sub, deduplication, and safe Iceberg writes

// --------------------------- // PART 1: Apache Beam Pipeline // ---------------------------

public class PubSubToGcsParquetPipeline {

public static void main(String[] args) {
    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();
    Pipeline pipeline = Pipeline.create(options);

    PCollection<YourRecord> parsed = pipeline
        .apply("ReadFromPubSub", PubsubIO.readStrings().fromSubscription("projects/YOUR_PROJECT/subscriptions/YOUR_SUB"))
        .apply("ParseJson", MapElements.into(TypeDescriptor.of(YourRecord.class))
            .via((String json) -> new Gson().fromJson(json, YourRecord.class)))
        .apply("Window", Window.<YourRecord>into(FixedWindows.of(Duration.standardMinutes(1)))
            .triggering(AfterWatermark.pastEndOfWindow())
            .withAllowedLateness(Duration.ZERO)
            .accumulatingFiredPanes());

    parsed.apply("WriteParquet", FileIO.<YourRecord>write()
        .via(ParquetIO.sink(YourRecord.class))
        .to("gs://your-bucket/tmp/iceberg-buffer/")
        .withPrefix("output-")
        .withSuffix(".parquet")
        .withNumShards(5));

    pipeline.run().waitUntilFinish();
}

// Sample POJO for ParquetIO
public static class YourRecord {
    public String id;
    public String value;
    public long timestamp;
    // Getters, Setters, Constructors
}

}

// ------------------------------------ // PART 2: Spark Job to Write to Iceberg (MERGE INTO) // ------------------------------------

public class IcebergSparkWriter {

public static void main(String[] args) throws Exception {

    String checkpointPath = "gs://your-bucket/checkpoints/last-folder.txt";
    String bufferBasePath = "gs://your-bucket/tmp/iceberg-buffer/";

    SparkSession spark = SparkSession.builder()
        .appName("IcebergWriter")
        .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkCatalog")
        .config("spark.sql.catalog.spark_catalog.type", "hadoop")
        .config("spark.sql.catalog.spark_catalog.warehouse", "gs://your-bucket/warehouse/")
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .getOrCreate();

    FileSystem fs = FileSystem.get(new URI(bufferBasePath), spark.sparkContext().hadoopConfiguration());
    Path latestFolder = findLatestFolder(fs, new Path(bufferBasePath), checkpointPath);

    if (latestFolder != null) {
        Dataset<Row> df = spark.read().parquet(latestFolder.toString());

        // Create temp view for SQL merge
        df.createOrReplaceTempView("staging_data");

        spark.sql("""
            MERGE INTO spark_catalog.db.iceberg_table t
            USING staging_data s
            ON t.id = s.id
            WHEN MATCHED THEN UPDATE SET *
            WHEN NOT MATCHED THEN INSERT *
        """);

        writeCheckpoint(fs, checkpointPath, latestFolder.toString());
    }

    spark.stop();
}

private static Path findLatestFolder(FileSystem fs, Path basePath, String checkpointPath) throws IOException {
    RemoteIterator<LocatedFileStatus> iter = fs.listFiles(basePath, true);
    Path latest = null;
    long latestTime = 0L;
    while (iter.hasNext()) {
        LocatedFileStatus file = iter.next();
        Path folder = file.getPath().getParent();
        long modTime = file.getModificationTime();
        if (modTime > latestTime) {
            latestTime = modTime;
            latest = folder;
        }
    }
    return latest;
}

private static void writeCheckpoint(FileSystem fs, String checkpointPath, String folder) throws IOException {
    FSDataOutputStream out = fs.create(new Path(checkpointPath), true);
    out.writeBytes(folder);
    out.close();
}

}

